{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import GEMA\n",
    "from GEMA import visualization\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from src import workflow as wf\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el conjunto de datos\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "data = pd.read_csv(url, na_values=missing_values, header=None)\n",
    "columna = 49\n",
    "#data[columna] = data[columna].replace({1: 0, 2: 1})\n",
    "\n",
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,columna)\n",
    "print(Smote_tomed[columna].value_counts())\n",
    "# Smote_tomed= Smote_tomed.reset_index(drop=True, inplace=True)\n",
    "# data_std= data_std.reset_index(drop=True, inplace=True)\n",
    "# diferentes=Smote_tomed[(Smote_tomed == data_std).all(axis=1) == False]\n",
    "# print(diferentes)\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [columna])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,columna)\n",
    "print(Smote_edited[columna].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [columna])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,columna)\n",
    "print(Smote_cnn[columna].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [columna])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,columna)\n",
    "print(Smote_ncr[columna].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [columna])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,columna)\n",
    "print(Smote_oss[columna].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [columna])\n",
    "data_smote_oss=data_smote_oss.to_numpy()\n",
    "\n",
    "\n",
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,columna)\n",
    "print(adasyn_tomek[columna].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [columna])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,columna)\n",
    "print(adasyn_edited[columna].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [columna])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn(data_std,columna)\n",
    "print(adasyn_cnn[columna].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [columna])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,columna)\n",
    "print(adasyn_ncr[columna].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [columna])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,columna)\n",
    "print(adasyn_oss[columna].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [columna])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,columna)\n",
    "print(bsmote_tomek[columna].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [columna])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,columna)\n",
    "print(bsmote_edited[columna].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [columna])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,columna)\n",
    "print(bsmote_cnn[columna].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [columna])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,columna)\n",
    "print(bsmote_ncr[columna].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [columna])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,columna)\n",
    "print(bsmote_oss[columna].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [columna])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,columna)\n",
    "print(svmsmote_tomek[columna].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [columna])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,columna)\n",
    "print(svmsmote_edited[columna].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [columna])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,columna)\n",
    "print(svmsmote_cnn[columna].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [columna])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,columna)\n",
    "print(svmsmote_ncr[columna].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [columna])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,columna)\n",
    "print(svmsmote_oss[columna].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [columna])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,columna)\n",
    "print(kmssmote_tomek[columna].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [columna])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,columna)\n",
    "print(kmssmote_edited[columna].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [columna])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,columna)\n",
    "print(kmssmote_cnn[columna].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [columna])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,columna)\n",
    "print(kmssmote_ncr[columna].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [columna])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, columna)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,columna)\n",
    "print(kmssmote_oss[columna].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [columna])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = Smote_edited.iloc[: , columna]\n",
    "X = Smote_edited.drop(Smote_tomed.columns[columna], axis=1)\n",
    "\n",
    "# Divide el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define los espacios de búsqueda para los hiperparámetros\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    hidden_layer_sizes = [int(trial.suggest_discrete_uniform(f'n_neurons_layer{i}', 10, 100, 10)) for i in range(2, 5)]\n",
    "    max_epochs = trial.suggest_int('max_epochs', 10, 500)\n",
    "\n",
    "    # Realiza validación cruzada k-fold (k=5)\n",
    "    cv_scores = cross_val_score(\n",
    "        MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate, max_iter=max_epochs, random_state=42),\n",
    "        X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calcula la media de los puntajes de validación\n",
    "    mean_accuracy = cv_scores.mean()\n",
    "\n",
    "    return mean_accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_hidden_layer_sizes = [best_params[f'n_neurons_layer{i}'] for i in range(2, 5)]\n",
    "best_max_epochs = best_params['max_epochs']\n",
    "\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(f\"Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Neuronas por capa: {best_hidden_layer_sizes}\")\n",
    "print(f\"Cantidad de épocas: {best_max_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes los siguientes valores para tu MLP\n",
    "learning_rate = 0.00019595629678249865\n",
    "epochs = 482\n",
    "hidden_layers = (20, 100, 50)\n",
    "\n",
    "y = Smote_tomed.iloc[: , columna]\n",
    "X = Smote_tomed.drop(adasyn_tomek.columns[columna], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Validación cruzada con k = 5\n",
    "kf = KFold(n_splits=5)\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    mlp_fold = MLPClassifier(hidden_layer_sizes=hidden_layers, learning_rate_init=learning_rate, max_iter=epochs)\n",
    "    mlp_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    train_pred = mlp_fold.predict(X_train_fold)\n",
    "    val_pred = mlp_fold.predict(X_val_fold)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train_fold, train_pred)\n",
    "    val_accuracy = accuracy_score(y_val_fold, val_pred)\n",
    "\n",
    "    # Almacenar los resultados de este fold en las listas\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Calcular las métricas en el conjunto de prueba\n",
    "test_pred = mlp_fold.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, test_pred)\n",
    "precision = precision_score(y_test, test_pred)\n",
    "sensitivity = recall_score(y_test, test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Calcular la media y desviación estándar de las métricas de entrenamiento y validación\n",
    "mean_train_accuracy = np.mean(train_accuracies)\n",
    "std_train_accuracy = np.std(train_accuracies)\n",
    "mean_val_accuracy = np.mean(val_accuracies)\n",
    "std_val_accuracy = np.std(val_accuracies)\n",
    "\n",
    "# Crear un archivo CSV y escribir los resultados\n",
    "nombre_archivo = \"resultsPetroleo.csv\"\n",
    "with open(nombre_archivo, 'a', newline='') as csvfile:\n",
    "    fieldnames = ['Experimento', 'Learning Rate', 'Épocas', 'Capas Usadas',\n",
    "                  'Training Accuracies', 'Validation Accuracies',\n",
    "                  'Mean Accuracy Training', 'Std Accuracy Training',\n",
    "                  'Mean Accuracy Validation', 'Std Accuracy Validation',\n",
    "                  'Accuracy Test', 'Precision Test', 'Sensibilidad Test', 'Especificidad Test']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writerow({\n",
    "        'Experimento': \"SMOTE+TL\",\n",
    "        'Learning Rate': learning_rate,\n",
    "        'Épocas': epochs,\n",
    "        'Capas Usadas': hidden_layers,\n",
    "        'Training Accuracies': train_accuracies,\n",
    "        'Validation Accuracies': val_accuracies,\n",
    "        'Mean Accuracy Training': mean_train_accuracy,\n",
    "        'Std Accuracy Training': std_train_accuracy,\n",
    "        'Mean Accuracy Validation': mean_val_accuracy,\n",
    "        'Std Accuracy Validation': std_val_accuracy,\n",
    "        'Accuracy Test': accuracy,\n",
    "        'Precision Test': precision,\n",
    "        'Sensibilidad Test': sensitivity,\n",
    "        'Especificidad Test': specificity\n",
    "    })\n",
    "\n",
    "print(f\"Resultados guardados en {nombre_archivo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
