{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "import missingpy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from GEMA import map\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from src import workflow as wf\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error que se produce en la representación de GEMA es cosa de la librería pero no impide seguir ejecutando las cosas. TAmbién es posible ver como se ha representado osea que no influye en nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "datos_mode[49].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [49])\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[49].values\n",
    "label_names={0:'Not_spiled', 1:'Spiled'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_som.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 16\n",
    "m_neurons = 16\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.24760602354016653, random_seed=42)\n",
    "som.train(data_som, num_iteration=10000, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(n_neurons, m_neurons))\n",
    "\n",
    "plt.pcolor(som.distance_map().T, cmap='Greys')  # plotting the distance map as background\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    9, \n",
    "                    7500, \n",
    "                    initial_lr=0.20, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'PCA')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(main_classification.quantization_error)\n",
    "print(main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMA.Visualization.codebook_vectors(main_map, np.array([\"Not Spiled\", \"Spiled\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,49)\n",
    "print(Smote_tomed[49].value_counts())\n",
    "# Smote_tomed= Smote_tomed.reset_index(drop=True, inplace=True)\n",
    "# data_std= data_std.reset_index(drop=True, inplace=True)\n",
    "# diferentes=Smote_tomed[(Smote_tomed == data_std).all(axis=1) == False]\n",
    "# print(diferentes)\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [49])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,49)\n",
    "print(Smote_edited[49].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [49])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,49)\n",
    "print(Smote_cnn[49].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [49])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,49)\n",
    "print(Smote_ncr[49].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [49])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,49)\n",
    "print(Smote_oss[49].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [49])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con GEMA y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificacion con Minisom y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,49)\n",
    "print(adasyn_tomek[49].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [49])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,49)\n",
    "print(adasyn_edited[49].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [49])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn(data_std,49)\n",
    "print(adasyn_cnn[49].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [49])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,49)\n",
    "print(adasyn_ncr[49].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [49])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,49)\n",
    "print(adasyn_oss[49].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [49])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con GEMA y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Minisom y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_adasyn_cnn), som.topographic_error(data_adasyn_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,49)\n",
    "print(bsmote_tomek[49].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [49])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,49)\n",
    "print(bsmote_edited[49].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [49])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,49)\n",
    "print(bsmote_cnn[49].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [49])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,49)\n",
    "print(bsmote_ncr[49].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [49])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,49)\n",
    "print(bsmote_oss[49].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [49])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con GEMA y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Minisom y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,49)\n",
    "print(svmsmote_tomek[49].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [49])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,49)\n",
    "print(svmsmote_edited[49].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [49])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,49)\n",
    "print(svmsmote_cnn[49].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [49])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,49)\n",
    "print(svmsmote_ncr[49].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [49])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,49)\n",
    "print(svmsmote_oss[49].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [49])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con GEMA y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Minisom y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,49)\n",
    "print(kmssmote_tomek[49].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [49])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,49)\n",
    "print(kmssmote_edited[49].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [49])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,49)\n",
    "print(kmssmote_cnn[49].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [49])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,49)\n",
    "print(kmssmote_ncr[49].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [49])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 49)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,49)\n",
    "print(kmssmote_oss[49].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [49])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con GEMA y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Minisom y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar dataset Cancer de mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/haberman.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)\n",
    "data[3] = data[3].replace({1: 0, 2: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "datos_mode[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [3])\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[3].values\n",
    "label_names={0:'plus 5', 1:'minus 5'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wf.min_max_normalization(data)\n",
    "X = X.drop(X.columns[3], axis=1)\n",
    "y = data.iloc[: , 3]\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=20))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=32, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    9, \n",
    "                    5000, \n",
    "                    initial_lr=0.3, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'sample')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(\"quantification error:\",main_classification.quantization_error)\n",
    "print(\"Topological_error:\",main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "porcentaje = 0.65\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((9, 9, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((9, 9))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((9, 9, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(data_som, target):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el 80% de ninguna clase\n",
    "\n",
    "\n",
    "# Count the number of neurons of each color\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "# ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((9, 9))\n",
    "ganadores = np.zeros((9, 9))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(data_som, target):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Minisom cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 7\n",
    "m_neurons = 7\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.23097207947323742, random_seed=42)\n",
    "som.train(data_som, num_iteration=5600, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_matrix = visualization.Visualization.umatrix(main_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMA.Visualization.heat_map(main_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Calcular la U-Matrix\n",
    "u_matrix = som.distance_map()\n",
    "\n",
    "# Visualizar la U-Matrix\n",
    "plt.imshow(u_matrix, cmap='gray', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Asigna colores basados en la cuarta columna (índice 3)\n",
    "data['color'] = ['blue' if x == 0 else 'orange' for x in data.iloc[:,3]]\n",
    "\n",
    "fig = px.scatter_3d(data, x=data.columns[0], y=data.columns[1], z=data.columns[2], color='color')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,3)\n",
    "print(Smote_tomed[3].value_counts())\n",
    "# Smote_tomed= Smote_tomed.reset_index(drop=True, inplace=True)\n",
    "# data_std= data_std.reset_index(drop=True, inplace=True)\n",
    "# diferentes=Smote_tomed[(Smote_tomed == data_std).all(axis=1) == False]\n",
    "# print(diferentes)\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [3])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_enn2(data_std,3)\n",
    "print(Smote_edited[3].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,3)\n",
    "print(Smote_cnn[3].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [3])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,3)\n",
    "print(Smote_ncr[3].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [3])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,3)\n",
    "print(Smote_oss[3].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [3])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Smote_tomed.iloc[: , 3]\n",
    "X = Smote_tomed.drop(Smote_tomed.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "pesos = model.get_weights()\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Smote_ncr.iloc[: , 3]\n",
    "X = Smote_ncr.drop(Smote_ncr.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "model.set_weights(pesos)\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con GEMA y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour, TomekLinks, EditedNearestNeighbours, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_enn_clase0(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = SMOTE(sampling_strategy={0: int(len(y[y==0])*1.5), 1: len(y[y==1])})\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = EditedNearestNeighbours(sampling_strategy='all', n_neighbors= 1)\n",
    "    pipeline = Pipeline([('SMOTE', smote), ('ENN', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "def smote_enn_clase1(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors= 1)\n",
    "    pipeline = Pipeline([('SMOTE', smote), ('ENN', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "Smote_enn_sinteticos0=smote_enn_clase0(data_std,3)\n",
    "Smote_enn_sinteticos1=smote_enn_clase1(data_std,3)\n",
    "smote_enn_sinteticos = pd.concat([Smote_enn_sinteticos0, Smote_enn_sinteticos1], axis=0, ignore_index=True)\n",
    "print(smote_enn_sinteticos.head(5))\n",
    "print(smote_enn_sinteticos[3].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adasyn_oss.iloc[: , 3]\n",
    "X = adasyn_oss.drop(adasyn_oss.columns[3], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "porcentaje = 0.7\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((9, 9, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((9, 9))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((9, 9, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(X, y):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el % de ninguna clase\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "#ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((9, 9))\n",
    "ganadores = np.zeros((9, 9))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(X, y):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")\n",
    "\n",
    "data_balanced_dropped_smote_edited_sint = smote_enn_sinteticos.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited_sint=data_balanced_dropped_smote_edited_sint.to_numpy()\n",
    "main_classification_smoted_enn_sinteticos = GEMA.Classification(main_map, data_balanced_dropped_smote_edited_sint)\n",
    "GEMA.Visualization.heat_map(main_classification_smoted_enn_sinteticos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con Minisom y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,3)\n",
    "print(adasyn_tomek[3].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [3])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,3)\n",
    "print(adasyn_edited[3].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [3])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn(data_std,3)\n",
    "print(adasyn_cnn[3].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [3])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,3)\n",
    "print(adasyn_ncr[3].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [3])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,3)\n",
    "print(adasyn_oss[3].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [3])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adasyn_tomek.iloc[: , 3]\n",
    "X = adasyn_tomek.drop(adasyn_tomek.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "pesos = model.get_weights()\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adasyn_oss.iloc[: , 3]\n",
    "X = adasyn_oss.drop(adasyn_oss.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "model.set_weights(pesos)\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con GEMA y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_adasyn_cnn= GEMA.Classification(main_map, data_adasyn_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_adasyn_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_adasyn_cnn.topological_error)\n",
    "\n",
    "main_classification_adasyn_ncr= GEMA.Classification(main_map, data_adasyn_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_adasyn_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_adasyn_ncr.topological_error)\n",
    "\n",
    "main_classification_adasyn_oss= GEMA.Classification(main_map, data_adasyn_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_adasyn_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_adasyn_oss.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_enn_clase0(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = ADASYN(sampling_strategy={0: int(len(y[y==0])*1.6), 1: len(y[y==1])})\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = NeighbourhoodCleaningRule(sampling_strategy='all', n_neighbors= 1)\n",
    "    pipeline = Pipeline([('SMOTE', smote), ('ENN', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "def smote_enn_clase1(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = ADASYN(sampling_strategy='auto')\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = NeighbourhoodCleaningRule(sampling_strategy='auto', n_neighbors= 1)\n",
    "    pipeline = Pipeline([('SMOTE', smote), ('ENN', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "Smote_enn_sinteticos0=smote_enn_clase0(data_std,3)\n",
    "Smote_enn_sinteticos1=smote_enn_clase1(data_std,3)\n",
    "smote_enn_sinteticos = pd.concat([Smote_enn_sinteticos0, Smote_enn_sinteticos1], axis=0, ignore_index=True)\n",
    "print(smote_enn_sinteticos.head(5))\n",
    "print(smote_enn_sinteticos[3].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = smote_enn_sinteticos.iloc[: , 3]\n",
    "X = smote_enn_sinteticos.drop(smote_enn_sinteticos.columns[3], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "porcentaje = 0.8\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((9, 9, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((9, 9))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((9, 9, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(X, y):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el 80% de ninguna clase\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "#ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((9, 9))\n",
    "ganadores = np.zeros((9, 9))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(X, y):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "data_balanced_dropped_smote_edited_sint = smote_enn_sinteticos.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited_sint=data_balanced_dropped_smote_edited_sint.to_numpy()\n",
    "main_classification_smoted_enn_sinteticos = GEMA.Classification(main_map, data_balanced_dropped_smote_edited_sint)\n",
    "GEMA.Visualization.heat_map(main_classification_smoted_enn_sinteticos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con Minisom y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_adasyn_cnn), som.topographic_error(data_adasyn_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,3)\n",
    "print(bsmote_tomek[3].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [3])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,3)\n",
    "print(bsmote_edited[3].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [3])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,3)\n",
    "print(bsmote_cnn[3].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [3])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,3)\n",
    "print(bsmote_ncr[3].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [3])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,3)\n",
    "print(bsmote_oss[3].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [3])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmssmote_oss.iloc[: , 3]\n",
    "X = kmssmote_oss.drop(kmssmote_oss.columns[3], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "porcentaje = 0.8\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((9, 9, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((9, 9))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((9, 9, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(X, y):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el % de ninguna clase\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "#ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((9, 9))\n",
    "ganadores = np.zeros((9, 9))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(X, y):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")\n",
    "\n",
    "data_balanced_dropped_smote_edited_sint = smote_enn_sinteticos.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited_sint=data_balanced_dropped_smote_edited_sint.to_numpy()\n",
    "main_classification_smoted_enn_sinteticos = GEMA.Classification(main_map, data_balanced_dropped_smote_edited_sint)\n",
    "GEMA.Visualization.heat_map(main_classification_smoted_enn_sinteticos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bsmote_tomek.iloc[: , 3]\n",
    "X = bsmote_tomek.drop(bsmote_tomek.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "pesos = model.get_weights()\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bsmote_oss.iloc[: , 3]\n",
    "X = bsmote_oss.drop(bsmote_oss.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "model.set_weights(pesos)\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con GEMA y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_enn_clase0(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = BorderlineSMOTE(sampling_strategy={0: int(len(y[y==0])*1.7), 1: len(y[y==1])})\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = TomekLinks(sampling_strategy='all')\n",
    "    pipeline = Pipeline([('BSMOTE', smote), ('TMK', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "def smote_enn_clase1(dataset, target):\n",
    "\n",
    "    print(dataset.head(5))\n",
    "    print(dataset[3].value_counts())   \n",
    "\n",
    "    # Separar las características y el target\n",
    "    X = dataset.drop(target, axis=1)\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Guardar los índices de las filas originales\n",
    "    original_indices = X.index\n",
    "\n",
    "    # Crear una instancia de SMOTE\n",
    "    smote = BorderlineSMOTE(sampling_strategy='auto')\n",
    "\n",
    "    # Crear una instancia de Condensed Nearest Neighbour\n",
    "    enn = TomekLinks(sampling_strategy='all')\n",
    "    pipeline = Pipeline([('BSMOTE', smote), ('TMK', enn)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "\n",
    "    # Redimensionamos las clases.\n",
    "    y_resampled = y_resampled.to_numpy().reshape(-1, 1)\n",
    "    y_resampled = y_resampled.reshape(np.size(X_resampled, 0),1)\n",
    "    \n",
    "    cols_df = dataset.columns\n",
    "    cols_df = cols_df.drop(target)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame(X_resampled, columns=cols_df)\n",
    "    synthetic_data[target] = y_resampled\n",
    "\n",
    "    # Eliminar las filas del conjunto de datos sobremuestreado que corresponden a las filas originales\n",
    "    synthetic_data_only = synthetic_data.drop(original_indices)\n",
    "                \n",
    "    return synthetic_data_only\n",
    "\n",
    "Smote_enn_sinteticos0=smote_enn_clase0(data_std,3)\n",
    "Smote_enn_sinteticos1=smote_enn_clase1(data_std,3)\n",
    "smote_enn_sinteticos = pd.concat([Smote_enn_sinteticos0, Smote_enn_sinteticos1], axis=0, ignore_index=True)\n",
    "print(smote_enn_sinteticos.head(5))\n",
    "print(smote_enn_sinteticos[3].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = smote_enn_sinteticos.iloc[: , 3]\n",
    "X = smote_enn_sinteticos.drop(smote_enn_sinteticos.columns[3], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "porcentaje = 0.8\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((9, 9, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((9, 9))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((9, 9, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(X, y):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el 80% de ninguna clase\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "#ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((9, 9))\n",
    "ganadores = np.zeros((9, 9))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(X, y):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "data_balanced_dropped_smote_edited_sint = smote_enn_sinteticos.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited_sint=data_balanced_dropped_smote_edited_sint.to_numpy()\n",
    "main_classification_smoted_enn_sinteticos = GEMA.Classification(main_map, data_balanced_dropped_smote_edited_sint)\n",
    "GEMA.Visualization.heat_map(main_classification_smoted_enn_sinteticos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con Minisom y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,3)\n",
    "print(svmsmote_tomek[3].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [3])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,3)\n",
    "print(svmsmote_edited[3].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [3])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,3)\n",
    "print(svmsmote_cnn[3].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [3])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,3)\n",
    "print(svmsmote_ncr[3].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [3])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,3)\n",
    "print(svmsmote_oss[3].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [3])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svmsmote_tomek.iloc[: , 3]\n",
    "X = svmsmote_tomek.drop(svmsmote_tomek.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "pesos = model.get_weights()\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svmsmote_oss.iloc[: , 3]\n",
    "X = svmsmote_oss.drop(svmsmote_oss.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "model.set_weights(pesos)\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con GEMA y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con Minisom y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans SMOTE cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,3)\n",
    "print(kmssmote_tomek[3].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [3])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,3)\n",
    "print(kmssmote_edited[3].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [3])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,3)\n",
    "print(kmssmote_cnn[3].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [3])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,3)\n",
    "print(kmssmote_ncr[3].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [3])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,3)\n",
    "print(kmssmote_oss[3].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [3])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmssmote_tomek.iloc[: , 3]\n",
    "X = kmssmote_tomek.drop(kmssmote_tomek.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "pesos = model.get_weights()\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmssmote_oss.iloc[: , 3]\n",
    "X = kmssmote_oss.drop(kmssmote_oss.columns[3], axis=1)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Escalar las características para mejorar el rendimiento del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valn = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crear un modelo secuencial en Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(units=45, activation='relu', input_dim=3))  # Capa de entrada con 20 características\n",
    "model.add(Dense(units=45, activation='relu'))  # Capa oculta con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Capa de salida con una neurona y función de activación sigmoide\n",
    "model.set_weights(pesos)\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Calcular Accuracy en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train) > 0.5)\n",
    "\n",
    "# Calcular Accuracy en el conjunto de validación\n",
    "validation_accuracy = accuracy_score(y_val, model.predict(X_val) > 0.5)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la especificidad\n",
    "TN = conf_matrix[0][0]  # Verdaderos negativos\n",
    "FP = conf_matrix[0][1]  # Falsos positivos\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall (Sensibility): {recall}')\n",
    "print(f'Specificity: {specificity}')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear la matriz de confusión gráfica\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con GEMA y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación cancer con Minisom y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar dataset de creditos bancarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)\n",
    "columns_to_encode = [0, 2, 3, 4, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19]\n",
    "\n",
    "# Inicializar el LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas seleccionadas\n",
    "for column in columns_to_encode:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "data[20] = data[20].replace({1: 0, 2: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = data_dropped\n",
    "datos_mode = wf.mode_imputation(data_dropped, 3)\n",
    "datos_mode[20].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [20])\n",
    "df = datos_mode.astype(int, errors='ignore')\n",
    "print(df)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[20].values\n",
    "label_names={0:'malo', 1:'bueno'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_som"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    13, \n",
    "                    5000, \n",
    "                    initial_lr=0.3, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'random')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(\"quantification error:\",main_classification.quantization_error)\n",
    "print(\"Topological_error:\",main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "porcentaje = 0.8\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((13, 13, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((13, 13))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((13, 13, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(data_som, target):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(13):\n",
    "    for j in range(13):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el 80% de ninguna clase\n",
    "\n",
    "\n",
    "# Count the number of neurons of each color\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "# ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((13, 13))\n",
    "ganadores = np.zeros((13, 13))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(data_som, target):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMA.Visualization.codebook_vectors(main_map, np.array([\"Not Spiled\", \"Spiled\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Minisom créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 17\n",
    "m_neurons = 17\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.27199933063716647, random_seed=42)\n",
    "som.train(data_som, num_iteration=9400, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,20)\n",
    "print(Smote_tomed[20].value_counts())\n",
    "# Smote_tomed= Smote_tomed.reset_index(drop=True, inplace=True)\n",
    "# data_std= data_std.reset_index(drop=True, inplace=True)\n",
    "# diferentes=Smote_tomed[(Smote_tomed == data_std).all(axis=1) == False]\n",
    "# print(diferentes)\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [20])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,20)\n",
    "print(Smote_edited[20].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [20])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,20)\n",
    "print(Smote_cnn[20].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [20])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,20)\n",
    "print(Smote_ncr[20].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [20])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,20)\n",
    "print(Smote_oss[20].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [20])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con GEMA y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con Minisom y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,20)\n",
    "print(adasyn_tomek[20].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [20])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,20)\n",
    "print(adasyn_edited[20].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [20])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn(data_std,20)\n",
    "print(adasyn_cnn[20].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [20])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,20)\n",
    "print(adasyn_ncr[20].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [20])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,20)\n",
    "print(adasyn_oss[20].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [20])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con GEMA y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con Minisom y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_adasyn_cnn), som.topographic_error(data_adasyn_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,20)\n",
    "print(bsmote_tomek[20].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [20])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,20)\n",
    "print(bsmote_edited[20].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [20])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,20)\n",
    "print(bsmote_cnn[20].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [20])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,20)\n",
    "print(bsmote_ncr[20].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [20])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,20)\n",
    "print(bsmote_oss[20].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [20])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con GEMA y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos Minisom y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,20)\n",
    "print(svmsmote_tomek[20].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [20])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,20)\n",
    "print(svmsmote_edited[20].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [20])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,20)\n",
    "print(svmsmote_cnn[20].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [20])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,20)\n",
    "print(svmsmote_ncr[20].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [20])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,20)\n",
    "print(svmsmote_oss[20].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [20])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos GEMA y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos Minisom y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans SMOTE créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,20)\n",
    "print(kmssmote_tomek[20].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [20])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,20)\n",
    "print(kmssmote_edited[20].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [20])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,20)\n",
    "print(kmssmote_cnn[20].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [20])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,20)\n",
    "print(kmssmote_ncr[20].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [20])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 20)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,20)\n",
    "print(kmssmote_oss[20].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [20])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con GEMA y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación créditos con Minisom y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/phoneme.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "datos_mode[5].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [5])\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[5].values\n",
    "label_names={0:'Nasals', 1:'Orals'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    19, \n",
    "                    1000, \n",
    "                    initial_lr=0.2, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'random')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(\"quantification error:\",main_classification.quantization_error)\n",
    "print(\"Topological_error:\",main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "porcentaje = 0.8\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((19, 19, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((19, 19))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((19, 19, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(data_som, target):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(19):\n",
    "    for j in range(19):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el 80% de ninguna clase\n",
    "\n",
    "\n",
    "# Count the number of neurons of each color\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "# ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((19, 19))\n",
    "ganadores = np.zeros((19, 19))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(data_som, target):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMA.Visualization.codebook_vectors(main_map, np.array([\"Not Spiled\", \"Spiled\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 6\n",
    "m_neurons = 6\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.12947086423703813, random_seed=42)\n",
    "som.train(data_som, num_iteration=4900, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,5)\n",
    "print(Smote_tomed[5].value_counts())\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [5])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,5)\n",
    "print(Smote_edited[5].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [5])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,5)\n",
    "print(Smote_cnn[5].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [5])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,5)\n",
    "print(Smote_ncr[5].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [5])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,5)\n",
    "print(Smote_oss[5].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [5])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con GEMA y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con Minisom y SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,5)\n",
    "print(adasyn_tomek[5].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [5])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,5)\n",
    "print(adasyn_edited[5].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [5])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn_fon(data_std,5)\n",
    "print(adasyn_cnn[5].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [5])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,5)\n",
    "print(adasyn_ncr[5].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [5])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,5)\n",
    "print(adasyn_oss[5].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [5])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con GEMA y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con Minisom y ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_adasyn_cnn), som.topographic_error(data_adasyn_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,5)\n",
    "print(bsmote_tomek[5].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [5])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,5)\n",
    "print(bsmote_edited[5].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [5])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,5)\n",
    "print(bsmote_cnn[5].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [5])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,5)\n",
    "print(bsmote_ncr[5].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [5])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,5)\n",
    "print(bsmote_oss[5].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [5])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con GEMA y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con Minisom y Borderline SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,5)\n",
    "print(svmsmote_tomek[5].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [5])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,5)\n",
    "print(svmsmote_edited[5].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [5])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,5)\n",
    "print(svmsmote_cnn[5].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [5])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,5)\n",
    "print(svmsmote_ncr[5].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [5])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,5)\n",
    "print(svmsmote_oss[5].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [5])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con GEMA y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con Minisom y SVM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans SMOTE fonemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,5)\n",
    "print(kmssmote_tomek[5].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [5])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,5)\n",
    "print(kmssmote_edited[5].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [5])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,5)\n",
    "print(kmssmote_cnn[5].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [5])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,5)\n",
    "print(kmssmote_ncr[5].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [5])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 5)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,5)\n",
    "print(kmssmote_oss[5].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [5])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con GEMA y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación fonemas con Minisom y Kmeans SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar dataset de fraudes de créditos bancarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar dataset\n",
    "url = '../../PROYECTO SOM DESBALANCEADO/DATASET/creditcard.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)\n",
    "data[3] = data[3].replace({1: 0, 2: 1})\n",
    "print(data[30].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "# Contar cuántas filas tienen un valor de 0 en la columna \"columna_nombre\"\n",
    "filas_con_cero = data[data[30] == 0]\n",
    "\n",
    "# Verificar si hay suficientes filas con 0 antes de eliminar\n",
    "if len(filas_con_cero) >= 274000:\n",
    "    # Eliminar las primeras 180,000 filas con 0\n",
    "    data = data.drop(filas_con_cero.index[:274000])\n",
    "datos_mode = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(datos_mode, 30)\n",
    "datos_mode[30].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [30])\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[30].values\n",
    "label_names={0:'No fraude', 1:'Fraude'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA fraudes créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    20, \n",
    "                    38386, \n",
    "                    initial_lr=0.01819940721573887, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'random')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(\"quantification error:\",main_classification.quantization_error)\n",
    "print(\"Topological_error:\",main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento SOM Minisom fraudes créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 20\n",
    "m_neurons = 20\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.005, random_seed=42)\n",
    "som.train(data_som, num_iteration=38386, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE fraudes créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,30)\n",
    "print(Smote_tomed[30].value_counts())\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [30])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,30)\n",
    "print(Smote_edited[30].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [30])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,30)\n",
    "print(Smote_cnn[30].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [30])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,30)\n",
    "print(Smote_ncr[30].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [30])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,30)\n",
    "print(Smote_oss[30].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [30])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,30)\n",
    "print(adasyn_tomek[30].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [30])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,30)\n",
    "print(adasyn_edited[30].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [30])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn_fon(data_std,30)\n",
    "print(adasyn_cnn[30].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [30])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,30)\n",
    "print(adasyn_ncr[30].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [30])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,30)\n",
    "print(adasyn_oss[30].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [30])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA ADASYN fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_adasyn_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_adasyn_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom ADASYN fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_adasyn_cnn), som.topographic_error(data_adasyn_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,30)\n",
    "print(bsmote_tomek[30].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [30])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,30)\n",
    "print(bsmote_edited[30].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [30])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,30)\n",
    "print(bsmote_cnn[30].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [30])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,30)\n",
    "print(bsmote_ncr[30].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [30])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,30)\n",
    "print(bsmote_oss[30].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [30])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA BSMOTE Fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom BSMOTE Fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,30)\n",
    "print(svmsmote_tomek[30].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [30])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,30)\n",
    "print(svmsmote_edited[30].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [30])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,30)\n",
    "print(svmsmote_cnn[30].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [30])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,30)\n",
    "print(svmsmote_ncr[30].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [30])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,30)\n",
    "print(svmsmote_oss[30].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [30])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svmsmote_ncr.iloc[: , 30]\n",
    "X = svmsmote_ncr.drop(svmsmote_ncr.columns[30], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "porcentaje = 0.8\n",
    "ladoMap = 20\n",
    "\n",
    "# Inicializa un mapa de colores del tamaño del SOM con todos los valores en blanco\n",
    "color_map = np.ones((ladoMap, ladoMap, 3))\n",
    "\n",
    "# Inicializa un mapa de clases del tamaño del SOM con todos los valores en -1\n",
    "class_map = -1 * np.ones((ladoMap, ladoMap))\n",
    "\n",
    "# Contadores para cada neurona\n",
    "counts = np.zeros((ladoMap, ladoMap, 2), dtype=int)\n",
    "\n",
    "# Itera sobre cada dato y su clase correspondiente\n",
    "for datum, class_ in zip(X, y):\n",
    "    # Encuentra la neurona ganadora para este dato\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    \n",
    "    # Actualiza el contador de la clase para la neurona ganadora\n",
    "    counts[int(winner[0]), int(winner[1]), int(class_)] += 1\n",
    "    \n",
    "    # Actualiza el mapa de clases con la clase reconocida\n",
    "    class_map[winner] = class_\n",
    "\n",
    "# Itera sobre cada neurona para determinar el color final\n",
    "for i in range(ladoMap):\n",
    "    for j in range(ladoMap):\n",
    "        # Calcula el porcentaje de cada clase para la neurona actual\n",
    "        percent_class_0 = counts[i, j, 0] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        percent_class_1 = counts[i, j, 1] / (counts[i, j, 0] + counts[i, j, 1]) if counts[i, j, 0] + counts[i, j, 1] > 0 else 0\n",
    "        \n",
    "        # Asigna el color según las condiciones especificadas\n",
    "        if percent_class_0 >= porcentaje:\n",
    "            color_map[i, j] = [0, 0, 1]  # Azul\n",
    "        elif percent_class_1 >= porcentaje:\n",
    "            color_map[i, j] = [1, 0, 0]  # Rojo\n",
    "        elif counts[i, j, 0] + counts[i, j, 1] == 0:\n",
    "            color_map[i, j] = [1, 1, 1]  # Blanco si no ha reconocido ningún patrón\n",
    "        else:\n",
    "            color_map[i, j] = [0, 1, 0]  # Verde si no alcanza el % de ninguna clase\n",
    "\n",
    "# Crea una figura con diferentes tamaños para los subplots\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Crea una leyenda para el primer gráfico\n",
    "legend_elements = [Patch(facecolor='blue', edgecolor='b', label='Clase 0'),\n",
    "                   Patch(facecolor='red', edgecolor='r', label='Clase 1'),\n",
    "                   Patch(facecolor='green', edgecolor='g', label='Ambas clases')]\n",
    "ax1 = fig.add_subplot(121)  # 121 significa \"1 fila, 2 columnas, primer gráfico\"\n",
    "#ax1.legend(handles=legend_elements, loc='upper right')\n",
    "ax1.imshow(np.rot90(color_map))\n",
    "ax1.set_title('Mapa de neuronas w/o escala')\n",
    "\n",
    "# Inicializa un mapa de neuronas con ceros\n",
    "neuron_map = np.zeros((ladoMap, ladoMap))\n",
    "ganadores = np.zeros((ladoMap, ladoMap))\n",
    "\n",
    "# Itera sobre los datos y las clases\n",
    "for datum, clss in zip(X, y):\n",
    "    # Obtiene la neurona ganadora para el dato actual\n",
    "    winner = main_map.calculate_bmu(datum)[1]\n",
    "    ganadores[winner] = 1\n",
    "    print(winner, clss)\n",
    "    # Si la clase es 0, resta 1, si la clase es 1, suma 1\n",
    "    neuron_map[winner[0], winner[1]] += 1 if clss == 1 else -1\n",
    "\n",
    "# Normaliza el mapa de neuronas para que los valores estén entre -1 y 1\n",
    "# neuron_map = neuron_map / np.max(np.abs(neuron_map))\n",
    "\n",
    "# Define los colores de la escala como azul, verde y amarillo\n",
    "cmap_colors = [\"indigo\", \"royalblue\", \"limegreen\", \"yellow\"]\n",
    "\n",
    "# Crea el mapa de colores\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", cmap_colors)\n",
    "\n",
    "# Dibuja el mapa de calor\n",
    "ax2 = fig.add_subplot(122)  # 122 significa \"1 fila, 2 columnas, segundo gráfico\"\n",
    "im = ax2.imshow(np.rot90(neuron_map), cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('Mapa de neuronas con escala')\n",
    "\n",
    "# Muestra la barra de colores\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "# Ajusta el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "num_blue = np.sum(np.all(color_map == [0, 0, 1], axis=-1))\n",
    "num_red = np.sum(np.all(color_map == [1, 0, 0], axis=-1))\n",
    "num_white = np.sum(np.all(color_map == [1, 1, 1], axis=-1))\n",
    "num_green = np.sum(np.all(color_map == [0, 1, 0], axis=-1))\n",
    "# Print the number of neurons of each color\n",
    "print(f\"Number of blue neurons: {num_blue}\")\n",
    "print(f\"Number of red neurons: {num_red}\")\n",
    "print(f\"Number of white neurons: {num_white}\")\n",
    "print(f\"Number of green neurons: {num_green}\")\n",
    "\n",
    "data_balanced_dropped_smote_edited_sint = smote_enn_sinteticos.drop(columns = [3])\n",
    "data_balanced_dropped_smote_edited_sint=data_balanced_dropped_smote_edited_sint.to_numpy()\n",
    "main_classification_smoted_enn_sinteticos = GEMA.Classification(main_map, data_balanced_dropped_smote_edited_sint)\n",
    "GEMA.Visualization.heat_map(main_classification_smoted_enn_sinteticos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación SVM SMOTE GEMA fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom SVM SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMS SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,30)\n",
    "print(kmssmote_tomek[30].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [30])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,30)\n",
    "print(kmssmote_edited[30].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [30])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,30)\n",
    "print(kmssmote_cnn[30].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [30])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,30)\n",
    "print(kmssmote_ncr[30].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [30])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 30)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,30)\n",
    "print(kmssmote_oss[30].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [30])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA KMS SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom KMS SMOTE fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar dataset\n",
    "url = '../../PROYECTO SOM DESBALANCEADO/DATASET/microcalcification.csv'\n",
    "missing_values = [' ', 'NaN', 'na', 'Na', '-', '--', 'n/a', '?']\n",
    "columns = ['class']\n",
    "data = pd.read_csv(url, na_values = missing_values, header=None)\n",
    "data = data.iloc[1:]\n",
    "data[6] = data[6].str.strip(\"'\").astype(int)\n",
    "data[6] = data[6].replace({-1: 0})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "'''# Contar cuántas filas tienen un valor de 0 en la columna \"columna_nombre\"\n",
    "filas_con_cero = data[data[30] == 0]\n",
    "\n",
    "# Verificar si hay suficientes filas con 0 antes de eliminar\n",
    "if len(filas_con_cero) >= 274000:\n",
    "    # Eliminar las primeras 180,000 filas con 0\n",
    "    data = data.drop(filas_con_cero.index[:274000])'''\n",
    "datos_mode = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(datos_mode, 6)\n",
    "datos_mode[6].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mode=datos_mode.drop(columns = [6])\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "target = data[6].values\n",
    "label_names={0:'Micro', 1:'No micro'}\n",
    "data_som=data_std.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento GEMA microcalcificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_map = GEMA.Map(data_som, \n",
    "                    5, \n",
    "                    43000, \n",
    "                    initial_lr=0.05275041385159045, \n",
    "                    initial_neighbourhood = 0,\n",
    "                    distance = 'euclidean',\n",
    "                    use_decay= False,\n",
    "                    presentation= 'random',\n",
    "                    weights = 'random')\n",
    "\n",
    "main_classification = GEMA.Classification(main_map, data_som)\n",
    "print(\"quantification error:\",main_classification.quantization_error)\n",
    "print(\"Topological_error:\",main_classification.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Minisom microcalcificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 5\n",
    "m_neurons = 5\n",
    "\n",
    "som = MiniSom(n_neurons, m_neurons, data_som.shape[1], learning_rate=0.05275041385159045, random_seed=42)\n",
    "som.train(data_som, num_iteration=43000, verbose=True)  # random training\n",
    "print(\"Topological error:\",som.topographic_error(data_som))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smote + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_tomed=wf.smote_tomed_link(data_std,6)\n",
    "print(Smote_tomed[6].value_counts())\n",
    "data_balanced_dropped_smote_tomed = Smote_tomed.drop(columns = [6])\n",
    "data_balanced_dropped_smote_tomed=data_balanced_dropped_smote_tomed.to_numpy()\n",
    "\n",
    "## smote_edited_nearest_neighbor\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_edited=wf.smote_edited_nearest_neighbor(data_std,6)\n",
    "print(Smote_edited[6].value_counts())\n",
    "data_balanced_dropped_smote_edited = Smote_edited.drop(columns = [6])\n",
    "data_balanced_dropped_smote_edited=data_balanced_dropped_smote_edited.to_numpy()\n",
    "\n",
    "## smote con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_cnn=wf.smote_cnn(data_std,6)\n",
    "print(Smote_cnn[6].value_counts())\n",
    "data_smote_cnn = Smote_cnn.drop(columns = [6])\n",
    "data_smote_cnn=data_smote_cnn.to_numpy()\n",
    "\n",
    "## smote con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_ncr=wf.smote_ncr(data_std,6)\n",
    "print(Smote_ncr[6].value_counts())\n",
    "data_smote_ncr = Smote_ncr.drop(columns = [6])\n",
    "data_smote_ncr=data_smote_ncr.to_numpy()\n",
    "\n",
    "## smote con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "Smote_oss=wf.smote_osd(data_std,6)\n",
    "print(Smote_oss[6].value_counts())\n",
    "data_smote_oss = Smote_oss.drop(columns = [6])\n",
    "data_smote_oss=data_smote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_smoted_tomed = GEMA.Classification(main_map, data_balanced_dropped_smote_tomed)\n",
    "print(\"Smote+tommed quantification:\",main_classification_smoted_tomed.quantization_error)\n",
    "print(\"Smote+tommed topological:\",main_classification_smoted_tomed.topological_error)\n",
    "\n",
    "main_classification_smoted_edited= GEMA.Classification(main_map, data_balanced_dropped_smote_edited)\n",
    "print(\"Smote+edited quantification:\",main_classification_smoted_edited.quantization_error)\n",
    "print(\"Smote+edited topological:\",main_classification_smoted_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"Smote+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"Smote+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_smote_ncr)\n",
    "print(\"Smote+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"Smote+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_smote_oss)\n",
    "print(\"Smote+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"Smote+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación SMOTE Minisom micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smote+tommed\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_tomed), som.topographic_error(data_balanced_dropped_smote_tomed))\n",
    "print(\"Smote+linked\")\n",
    "print(som.quantization_error(data_balanced_dropped_smote_edited), som.topographic_error(data_balanced_dropped_smote_edited))\n",
    "print(\"Smote+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"Smote+ncr\")\n",
    "print(som.quantization_error(data_smote_ncr), som.topographic_error(data_smote_ncr))\n",
    "print(\"Smote+oss\")\n",
    "print(som.quantization_error(data_smote_oss), som.topographic_error(data_smote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADASYN + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_tomek=wf.adasyn_tomek(data_std,6)\n",
    "print(adasyn_tomek[6].value_counts())\n",
    "data_adasyn_tomek = adasyn_tomek.drop(columns = [6])\n",
    "data_adasyn_tomek=data_adasyn_tomek.to_numpy()\n",
    "\n",
    "## ADASYN + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_edited=wf.adasyn_enn(data_std,6)\n",
    "print(adasyn_edited[6].value_counts())\n",
    "data_adasyn_edited = adasyn_edited.drop(columns = [6])\n",
    "data_adasyn_edited=data_adasyn_edited.to_numpy()\n",
    "\n",
    "## ADASYN con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_cnn=wf.adasyn_cnn(data_std,6)\n",
    "print(adasyn_cnn[6].value_counts())\n",
    "data_adasyn_cnn = adasyn_cnn.drop(columns = [6])\n",
    "data_adasyn_cnn=data_adasyn_cnn.to_numpy()\n",
    "\n",
    "## ADASYN con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_ncr=wf.adasyn_ncr(data_std,6)\n",
    "print(adasyn_ncr[6].value_counts())\n",
    "data_adasyn_ncr = adasyn_ncr.drop(columns = [6])\n",
    "data_adasyn_ncr=data_adasyn_ncr.to_numpy()\n",
    "\n",
    "## ADASYN con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "adasyn_oss=wf.adasyn_oss(data_std,6)\n",
    "print(adasyn_oss[6].value_counts())\n",
    "data_adasyn_oss = adasyn_oss.drop(columns = [6])\n",
    "data_adasyn_oss=data_adasyn_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificacion GEMA ADASYN Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_adasyn_tomek = GEMA.Classification(main_map, data_adasyn_tomek)\n",
    "print(\"adasyn+tommed quantification:\",main_classification_adasyn_tomek.quantization_error)\n",
    "print(\"adasyn+tommed topological:\",main_classification_adasyn_tomek.topological_error)\n",
    "\n",
    "main_classification_adasyn_edited= GEMA.Classification(main_map, data_adasyn_edited)\n",
    "print(\"adasyn+edited quantification:\",main_classification_adasyn_edited.quantization_error)\n",
    "print(\"adasyn+edited topological:\",main_classification_adasyn_edited.topological_error)\n",
    "\n",
    "main_classification_smoted_cnn= GEMA.Classification(main_map, data_smote_cnn)\n",
    "print(\"adasyn+cnn quantification:\",main_classification_smoted_cnn.quantization_error)\n",
    "print(\"adasyn+cnn topological:\",main_classification_smoted_cnn.topological_error)\n",
    "\n",
    "main_classification_smoted_ncr= GEMA.Classification(main_map, data_adasyn_ncr)\n",
    "print(\"adasyn+ncr quantification:\",main_classification_smoted_ncr.quantization_error)\n",
    "print(\"adasyn+ncr topological:\",main_classification_smoted_ncr.topological_error)\n",
    "\n",
    "main_classification_smoted_oss= GEMA.Classification(main_map, data_adasyn_oss)\n",
    "print(\"adasyn+oss quantification:\",main_classification_smoted_oss.quantization_error)\n",
    "print(\"adasyn+oss topological:\",main_classification_smoted_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom ADASYN Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adasyn+tommed\")\n",
    "print(som.quantization_error(data_adasyn_tomek), som.topographic_error(data_adasyn_tomek))\n",
    "print(\"adasyn+linked\")\n",
    "print(som.quantization_error(data_adasyn_edited), som.topographic_error(data_adasyn_edited))\n",
    "print(\"adasyn+cnn\")\n",
    "print(som.quantization_error(data_smote_cnn), som.topographic_error(data_smote_cnn))\n",
    "print(\"adasyn+ncr\")\n",
    "print(som.quantization_error(data_adasyn_ncr), som.topographic_error(data_adasyn_ncr))\n",
    "print(\"adasyn+oss\")\n",
    "print(som.quantization_error(data_adasyn_oss), som.topographic_error(data_adasyn_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_tomek=wf.BSMOTE_tomek(data_std,6)\n",
    "print(bsmote_tomek[6].value_counts())\n",
    "data_bsmote_tomek = bsmote_tomek.drop(columns = [6])\n",
    "data_bsmote_tomek=data_bsmote_tomek.to_numpy()\n",
    "\n",
    "## BSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_edited=wf.BSMOTE_enn(data_std,6)\n",
    "print(bsmote_edited[6].value_counts())\n",
    "data_bsmote_edited = bsmote_edited.drop(columns = [6])\n",
    "data_bsmote_edited=data_bsmote_edited.to_numpy()\n",
    "\n",
    "## BSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_cnn=wf.BSMOTE_cnn(data_std,6)\n",
    "print(bsmote_cnn[6].value_counts())\n",
    "data_bsmote_cnn = bsmote_cnn.drop(columns = [6])\n",
    "data_bsmote_cnn=data_bsmote_cnn.to_numpy()\n",
    "\n",
    "## BSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_ncr=wf.BSMOTE_ncr(data_std,6)\n",
    "print(bsmote_ncr[6].value_counts())\n",
    "data_bsmote_ncr = bsmote_ncr.drop(columns = [6])\n",
    "data_bsmote_ncr=data_bsmote_ncr.to_numpy()\n",
    "\n",
    "## BSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "bsmote_oss=wf.BSMOTE_oss(data_std,6)\n",
    "print(bsmote_oss[6].value_counts())\n",
    "data_bsmote_oss = bsmote_oss.drop(columns = [6])\n",
    "data_bsmote_oss=data_bsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA BSMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_bsmote_tomek = GEMA.Classification(main_map, data_bsmote_tomek)\n",
    "print(\"bsmote+tommed quantification:\",main_classification_bsmote_tomek.quantization_error)\n",
    "print(\"bsmote+tommed topological:\",main_classification_bsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_bsmote_edited= GEMA.Classification(main_map, data_bsmote_edited)\n",
    "print(\"bsmote+edited quantification:\",main_classification_bsmote_edited.quantization_error)\n",
    "print(\"bsmote+edited topological:\",main_classification_bsmote_edited.topological_error)\n",
    "\n",
    "main_classification_bsmote_cnn= GEMA.Classification(main_map, data_bsmote_cnn)\n",
    "print(\"bsmote+cnn quantification:\",main_classification_bsmote_cnn.quantization_error)\n",
    "print(\"bsmote+cnn topological:\",main_classification_bsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_bsmote_ncr= GEMA.Classification(main_map, data_bsmote_ncr)\n",
    "print(\"bsmote+ncr quantification:\",main_classification_bsmote_ncr.quantization_error)\n",
    "print(\"bsmote+ncr topological:\",main_classification_bsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_bsmote_oss= GEMA.Classification(main_map, data_bsmote_oss)\n",
    "print(\"bsmote+oss quantification:\",main_classification_bsmote_oss.quantization_error)\n",
    "print(\"bsmote+oss topological:\",main_classification_bsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación Minisom BSMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bsmote+tommed\")\n",
    "print(som.quantization_error(data_bsmote_tomek), som.topographic_error(data_bsmote_tomek))\n",
    "print(\"bsmote+linked\")\n",
    "print(som.quantization_error(data_bsmote_edited), som.topographic_error(data_bsmote_edited))\n",
    "print(\"bsmote+cnn\")\n",
    "print(som.quantization_error(data_bsmote_cnn), som.topographic_error(data_bsmote_cnn))\n",
    "print(\"bsmote+ncr\")\n",
    "print(som.quantization_error(data_bsmote_ncr), som.topographic_error(data_bsmote_ncr))\n",
    "print(\"bsmote+oss\")\n",
    "print(som.quantization_error(data_bsmote_oss), som.topographic_error(data_bsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVMSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_tomek=wf.SVMSMOTE_tomek(data_std,6)\n",
    "print(svmsmote_tomek[6].value_counts())\n",
    "data_svmsmote_tomek = svmsmote_tomek.drop(columns = [6])\n",
    "data_svmsmote_tomek=data_svmsmote_tomek.to_numpy()\n",
    "\n",
    "## SVMSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_edited=wf.SVMSMOTE_enn(data_std,6)\n",
    "print(svmsmote_edited[6].value_counts())\n",
    "data_svmsmote_edited = svmsmote_edited.drop(columns = [6])\n",
    "data_svmsmote_edited=data_svmsmote_edited.to_numpy()\n",
    "\n",
    "## SVMSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_cnn=wf.SVMSMOTE_cnn(data_std,6)\n",
    "print(svmsmote_cnn[6].value_counts())\n",
    "data_svmsmote_cnn = svmsmote_cnn.drop(columns = [6])\n",
    "data_svmsmote_cnn=data_svmsmote_cnn.to_numpy()\n",
    "\n",
    "## SVMSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_ncr=wf.SVMSMOTE_ncr(data_std,6)\n",
    "print(svmsmote_ncr[6].value_counts())\n",
    "data_svmsmote_ncr = svmsmote_ncr.drop(columns = [6])\n",
    "data_svmsmote_ncr=data_svmsmote_ncr.to_numpy()\n",
    "\n",
    "## SVMSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "svmsmote_oss=wf.SVMSMOTE_oss(data_std,6)\n",
    "print(svmsmote_oss[6].value_counts())\n",
    "data_svmsmote_oss = svmsmote_oss.drop(columns = [6])\n",
    "data_svmsmote_oss=data_svmsmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA SVM SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_svmsmote_tomek = GEMA.Classification(main_map, data_svmsmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_svmsmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_svmsmote_tomek.topological_error)\n",
    "\n",
    "main_classification_svmsmote_edited= GEMA.Classification(main_map, data_svmsmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_svmsmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_svmsmote_edited.topological_error)\n",
    "\n",
    "main_classification_svmsmote_cnn= GEMA.Classification(main_map, data_svmsmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_svmsmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_svmsmote_cnn.topological_error)\n",
    "\n",
    "main_classification_svmsmote_ncr= GEMA.Classification(main_map, data_svmsmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_svmsmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_svmsmote_ncr.topological_error)\n",
    "\n",
    "main_classification_svmsmote_oss= GEMA.Classification(main_map, data_svmsmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_svmsmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_svmsmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación SVM SMOTE Minisom Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svmsmote+tommed\")\n",
    "print(som.quantization_error(data_svmsmote_tomek), som.topographic_error(data_svmsmote_tomek))\n",
    "print(\"svmsmote+linked\")\n",
    "print(som.quantization_error(data_svmsmote_edited), som.topographic_error(data_svmsmote_edited))\n",
    "print(\"svmsmote+cnn\")\n",
    "print(som.quantization_error(data_svmsmote_cnn), som.topographic_error(data_svmsmote_cnn))\n",
    "print(\"svmsmote+ncr\")\n",
    "print(som.quantization_error(data_svmsmote_ncr), som.topographic_error(data_svmsmote_ncr))\n",
    "print(\"svmsmote+oss\")\n",
    "print(som.quantization_error(data_svmsmote_oss), som.topographic_error(data_svmsmote_oss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMS SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMSSMOTE + Tomed_links\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_tomek=wf.KMSSMOTE_tomek(data_std,6)\n",
    "print(kmssmote_tomek[6].value_counts())\n",
    "data_kmssmote_tomek = kmssmote_tomek.drop(columns = [6])\n",
    "data_kmssmote_tomek=data_kmssmote_tomek.to_numpy()\n",
    "\n",
    "## KMSSMOTE + ENN\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_edited=wf.KMSSMOTE_enn(data_std,6)\n",
    "print(kmssmote_edited[6].value_counts())\n",
    "data_kmssmote_edited = kmssmote_edited.drop(columns = [6])\n",
    "data_kmssmote_edited=data_kmssmote_edited.to_numpy()\n",
    "\n",
    "## KMSSMOTE con CNN (condensed nearest neighbors)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_cnn=wf.KMSSMOTE_cnn(data_std,6)\n",
    "print(kmssmote_cnn[6].value_counts())\n",
    "data_kmssmote_cnn = kmssmote_cnn.drop(columns = [6])\n",
    "data_kmssmote_cnn=data_kmssmote_cnn.to_numpy()\n",
    "\n",
    "## KMSSMOTE con NCR (Neighboorhood cleaning rule)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_ncr=wf.KMSSMOTE_ncr(data_std,6)\n",
    "print(kmssmote_ncr[6].value_counts())\n",
    "data_kmssmote_ncr = kmssmote_ncr.drop(columns = [6])\n",
    "data_kmssmote_ncr=data_kmssmote_ncr.to_numpy()\n",
    "\n",
    "## KMSSMOTE con OSD (One sided selection)\n",
    "data_dropped = wf.drop_missing_values_columns(data, 5)\n",
    "datos_mode = wf.mode_imputation(data_dropped, 6)\n",
    "data_std = wf.min_max_normalization(datos_mode)\n",
    "\n",
    "kmssmote_oss=wf.KMSSMOTE_oss(data_std,6)\n",
    "print(kmssmote_oss[6].value_counts())\n",
    "data_kmssmote_oss = kmssmote_oss.drop(columns = [6])\n",
    "data_kmssmote_oss=data_kmssmote_oss.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación GEMA KMS SMOTE Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classification_kmssmote_tomek = GEMA.Classification(main_map, data_kmssmote_tomek)\n",
    "print(\"svmsmote+tommed quantification:\",main_classification_kmssmote_tomek.quantization_error)\n",
    "print(\"svmsmote+tommed topological:\",main_classification_kmssmote_tomek.topological_error)\n",
    "\n",
    "main_classification_kmssmote_edited= GEMA.Classification(main_map, data_kmssmote_edited)\n",
    "print(\"svmsmote+edited quantification:\",main_classification_kmssmote_edited.quantization_error)\n",
    "print(\"svmsmote+edited topological:\",main_classification_kmssmote_edited.topological_error)\n",
    "\n",
    "main_classification_kmssmote_cnn= GEMA.Classification(main_map, data_kmssmote_cnn)\n",
    "print(\"svmsmote+cnn quantification:\",main_classification_kmssmote_cnn.quantization_error)\n",
    "print(\"svmsmote+cnn topological:\",main_classification_kmssmote_cnn.topological_error)\n",
    "\n",
    "main_classification_kmssmote_ncr= GEMA.Classification(main_map, data_kmssmote_ncr)\n",
    "print(\"svmsmote+ncr quantification:\",main_classification_kmssmote_ncr.quantization_error)\n",
    "print(\"svmsmote+ncr topological:\",main_classification_kmssmote_ncr.topological_error)\n",
    "\n",
    "main_classification_kmssmote_oss= GEMA.Classification(main_map, data_kmssmote_oss)\n",
    "print(\"svmsmote+oss quantification:\",main_classification_kmssmote_oss.quantization_error)\n",
    "print(\"svmsmote+oss topological:\",main_classification_kmssmote_oss.topological_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación KMS SMOTE Minisom Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmssmote+tommed\")\n",
    "print(som.quantization_error(data_kmssmote_tomek), som.topographic_error(data_kmssmote_tomek))\n",
    "print(\"kmssmote+linked\")\n",
    "print(som.quantization_error(data_kmssmote_edited), som.topographic_error(data_kmssmote_edited))\n",
    "print(\"kmssmote+cnn\")\n",
    "print(som.quantization_error(data_kmssmote_cnn), som.topographic_error(data_kmssmote_cnn))\n",
    "print(\"kmssmote+ncr\")\n",
    "print(som.quantization_error(data_kmssmote_ncr), som.topographic_error(data_kmssmote_ncr))\n",
    "print(\"kmssmote+oss\")\n",
    "print(som.quantization_error(data_kmssmote_oss), som.topographic_error(data_kmssmote_oss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Ceiec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "add33ec30ad07fb172a3e5d915360277bab4a596f3212f7a8edc62cdefc58d7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
